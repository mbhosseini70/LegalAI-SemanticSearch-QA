{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10246115,"sourceType":"datasetVersion","datasetId":6336885}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# For a detailed explanation of the code and additional information, please visit this GitHub repository:\n\nhttps://github.com/mbhosseini70/LegalAI-SemanticSearch-QA\n","metadata":{}},{"cell_type":"code","source":"! pip install langchain\n!pip install -qU langchain-google-genai\n\n! pip install chromadb\n\n!pip install -qU langchain_community pypdf\n\n! pip install -U langchain-cohere\n\n!pip install --upgrade chromadb\n\n!pip install -U huggingface-hub sentence-transformers chromadb cohere transformers\n","metadata":{"id":"gmpBo5oOLSZa","outputId":"debec5fa-3cd6-435d-8d11-469ddbadb2b2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade sympy\n","metadata":{"id":"cXyBkFvILZfn","outputId":"09a9c90d-f06b-41eb-feab-d00cdf97fbd5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import getpass\nimport os\nos.environ[\"COHERE_API_KEY\"] = \"XXXXXX\"\n\n\nfrom huggingface_hub import login\nlogin(\"XXXXXYYYYYYYY\")","metadata":{"id":"EKdhpmhlLZZY"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## almost working","metadata":{"id":"X454gXqWKhBw"}},{"cell_type":"code","source":"import os\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.document_loaders import TextLoader\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.prompts.chat import ChatPromptTemplate\nfrom langchain.chains.combine_documents.stuff import create_stuff_documents_chain\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom langchain_cohere import ChatCohere\nfrom transformers import AutoModel, AutoTokenizer\nfrom langchain.embeddings import HuggingFaceEmbeddings\nimport shutil\nfrom transformers import AutoTokenizer, AutoModel\nfrom sentence_transformers import SentenceTransformer\n\nfrom langchain.schema import HumanMessage\nimport time\n\nclass LegalBERTEmbeddings:\n    def __init__(self):\n        self.model = SentenceTransformer(\"nlpaueb/legal-bert-base-uncased\")\n        print(\"LegalBERT model loaded successfully.\")\n\n    def embed(self, texts):\n        \"\"\"Return embeddings for a list of texts.\"\"\"\n        if isinstance(texts, str):\n            texts = [texts]\n        return self.model.encode(texts, batch_size=32).tolist()\n\n\n\n\n\nclass DocumentProcessor:\n    def __init__(self, collection_name, persist_directory, embeddings_model):\n        self.collection_name = collection_name\n        self.persist_directory = persist_directory\n        self.embeddings_model = embeddings_model\n        self.vector_store = None\n        self.documents = []\n        self.llm = ChatCohere()\n        print(f\"DocumentProcessor initialized with collection_name: {collection_name}\")\n\n    def load_files(self, file_path):\n        documents = []\n\n        # Load and split documents\n        if file_path.lower().endswith('.pdf'):\n            loader = PyPDFLoader(file_path)\n            documents.extend(loader.load())\n        elif file_path.lower().endswith('.txt'):\n            loader = TextLoader(file_path)\n            documents.extend(loader.load())\n\n\n        # Split documents into chunks\n        splitter = RecursiveCharacterTextSplitter(\n            separators=[\"\\n\\n\"],\n            chunk_size=250,\n            chunk_overlap=100,\n            length_function=len\n        )\n        splitted_docs_with_titles = []\n        splitted_docs_without_titles = []\n\n        for doc in documents:\n            chunks = splitter.split_text(doc.page_content)\n            for chunk in chunks:\n                title = self._generate_questions(chunk)\n\n                # Save chunk with title\n                chunk_with_title = f\"Title: {title}\\n\\n{chunk}\"\n\n                # Remove \"Title:\" and \"Summary:\" keywords\n                cleaned_chunk_with_title = chunk_with_title.replace(\"Title:\", \"\").replace(\"Summary:\", \"\").strip()\n\n                splitted_docs_with_titles.append(Document(page_content=cleaned_chunk_with_title))\n\n                # Save chunk without title\n                # splitted_docs_without_titles.append(Document(page_content=chunk, metadata=doc.metadata))\n\n        # print(f\"Total chunks created with titles: {len(splitted_docs_with_titles)}\")\n        # print(f\"Total chunks created without titles: {len(splitted_docs_without_titles)}\")\n\n        # Combine both versions into the same vector store\n        # all_docs = splitted_docs_with_titles + splitted_docs_without_titles\n        all_docs = splitted_docs_with_titles\n\n        # # Showing what is going to be saved in the database\n        # for idx, doc in enumerate(all_docs):\n        #     print(f\"\\n--- Chunk {idx + 1} ---\")\n        #     print(f\"Content: {doc.page_content[:200]}...\")\n        #     # print(f\"Metadata: {doc.metadata}\")\n\n        # Save the documents to class attribute\n        self.documents = all_docs\n\n        # Initialize vector store\n        self.vector_store = Chroma.from_documents(\n            documents=self.documents,\n            embedding=HuggingFaceEmbeddings(model_name=\"nlpaueb/legal-bert-base-uncased\"),\n            collection_name=self.collection_name,\n            persist_directory=self.persist_directory\n        )\n        print(\"Vector store created and persisted successfully.\")\n\n\n    def _generate_questions(self, chunk):\n\n\n\n        try:\n            # Few-shot examples\n            examples = (\n                \"Example 1:\\n\"\n                \"Text:\\n\"\n                \"12 October 2023\\n\"\n                \"LEASE AGREEMENT UNDER COMMON LAW\\n\"\n                \"between\\n\"\n                \"Huppeldepup NV as Lessor\\n\"\n                \"and\\n\"\n                \"Calimero BV as Lessee\\n\\n\"\n                \"Summary: the names of the lessor and lessee.\\n\\n\"\n\n                \"Example 2:\\n\"\n                \"Text:\\n\"\n                \"2 Object of lease\\n\"\n                \"The Lessor leases to the Lessee, who accepts, the real estate located at Stationsstraat 12, 2590 Berlaar Kontich, \"\n                \"with an equipped office building with a total usable floor area of 500m2 spread over three levels, as well as eleven parking spaces (outdoor) \"\n                \"with seven electric charging stations, (the Property), under the common law lease system and under the conditions and terms described in this Agreement.\\n\\n\"\n                \"Summary: The address of the property and its facilities.\\n\\n\"\n\n                \"Example 3:\\n\"\n                \"Text:\\n\"\n                \"AND:\\n\"\n                \"2. Calimero BV, a private limited company under Belgian law with registered office at Stationsstraat 12, 2590 Berlaar, \"\n                \"and registered in the Legal Entities Register under number 0222.222.222 (RPR Antwerp, Mechelen division), \"\n                \"hereinafter referred to as the Lessee.\\n\\n\"\n                \"Summary: Name of the Lessee (tenant) and their information and address.\\n\\n\"\n            )\n\n            # Define the prompt with few-shot examples\n            prompt = (\n                \"You are an expert in analyzing legal texts and generating concise, contextually appropriate summary titles for semantic search. \"\n                \"Your task is to identify key information from legal contracts, such as tenant and landlord roles and names, goods or property price and address, \"\n                \"and other relevant aspects. \"\n                \"Use precise legal terminology, avoid specific names, entities, or dates, and ensure the title is short, general, and accurately reflects the main content.\\n\\n\"\n                f\"{examples}\"\n                f\"Text:\\n{chunk}\\n\\n\"\n                \"Summary:\"\n            )\n\n            # Prepare the message format for the generate method\n            messages = [[HumanMessage(content=prompt)]]\n\n            # print(f\"\\n--- Processing Chunk ---\\n{chunk[:200]}...\")  # Display the first 200 characters of the chunk for readability\n\n            # Call the Cohere LLM's generate method with a delay\n            response = self.llm.generate(messages=messages)\n\n            # Introduce a 6-second delay to prevent rate limiting\n            time.sleep(6)\n\n            # Extract the generated title from the LLMResult\n            if response.generations and len(response.generations) > 0:\n                title = response.generations[0][0].text.strip()\n                # print(f\"--- Generated Title ---\\n{title}\\n\")  # Display the generated title\n                return title\n            else:\n                raise ValueError(\"No generations returned by the LLM.\")\n\n        except Exception as e:\n            print(f\"Error generating title: {e}\")\n            return f\"No title generated due to an error: {str(e)}\"\n\n\n\n\n\n    def retrieve_with_cosine_similarity(self, query):\n\n        query_embedding = np.array(self.embeddings_model.embed(query))\n\n        all_embeddings = [\n            np.array(self.embeddings_model.embed(doc.page_content))\n            for doc in self.documents\n        ]\n        all_embeddings = np.vstack(all_embeddings)\n\n        # Calculate cosine similarities\n        similarities = cosine_similarity(query_embedding, all_embeddings)[0]\n\n        sorted_indices = np.argsort(similarities)[::-1]\n        sorted_docs = [self.documents[i] for i in sorted_indices]\n\n        # Print chunks with similarity scores\n        # for i, doc in enumerate(sorted_docs[:5]):\n        #     print(f\"Chunk {i+1}: {doc.page_content}...\")\n        #     print(f\"Similarity Score: {similarities[sorted_indices[i]]}\")\n        #     print(\"---\" * 50)\n\n        return sorted_docs[:5]\n\n\nclass QAChain:\n    def __init__(self, document_processor):\n        self.document_processor = document_processor\n        self.llm = ChatCohere()\n        self.chain = self._initialize_chain()\n\n\n    def _initialize_chain(self):\n        print(\"Initializing retrieval chain and LLM...\")\n        system_prompt = (\n            \"You are a legal specialist, Use the information provided below to answer the question precisely. \"\n            \"Follow these rules: \"\n            \"1. Provide the exact answer as it appears in the provided content. Never ever rephrase, interpret, or expand on the answer. \"\n            \"2. If additional information relevant to the answer is available, provide it after the exact answer. Never ever rephrase, interpret, or expand on the answer.\"\n            \"3. If you cannot find the answer in the provided content, respond with: 'I could not find the answer.' \"\n            \"4. If the provided content or question is in another European language, answer in that language. \"\n            \"5. If you cannot recognize the language or respond in it, state: 'Please use English or another European language.' \"\n            \"Context: {context}\"\n        )\n\n        prompt = ChatPromptTemplate.from_messages([\n            (\"system\", system_prompt),\n            (\"human\", \"{input}\"),\n        ])\n        question_answer_chain = create_stuff_documents_chain(self.llm, prompt)\n\n        return create_retrieval_chain(\n            retriever=self.document_processor.vector_store.as_retriever(),\n            combine_docs_chain=question_answer_chain\n        )\n\n    def run_queries(self, queries):\n        responses = []\n        for query in queries:\n            # print(f\"Running query: '{query}'\")\n            try:\n                retrieved_docs = self.document_processor.retrieve_with_cosine_similarity(query)\n                context = \" \".join(doc.page_content for doc in retrieved_docs[:3])\n                # print(f\"Context passed to LLM for query '{query}':\\n{context}\\n{'-'*50}\")\n                # print(context)\n\n                response = self.chain.invoke({\"input\": query, \"context\": context})\n                time.sleep(6)\n                # print(f\"LLM response: {response}\")\n\n                responses.append({\n                    \"query\": query,\n                    \"answer\": response.get(\"answer\", \"I could not find the answer.\")\n                })\n            except Exception as e:\n                # print(f\"Error during query: {e}\")\n                responses.append({\n                    \"query\": query,\n                    \"answer\": \"Error during query.\"\n                })\n        return responses\n\n\n\n\n# Function to Run Queries Independently\ndef run_independent_queries(queries):\n    \"\"\"\n    Function to execute queries without restarting the document processing steps.\n    \"\"\"\n    print(\"\\n\" + \"=\" * 70)\n    print(\"Executing Queries and Displaying Results\")\n    print(\"=\" * 70 + \"\\n\")\n\n    results = qa_chain.run_queries(queries)\n\n    for i, result in enumerate(results, start=1):\n        print(f\"\\nQuery {i}/{len(results)}\")\n        print(\"-\" * 70)\n        print(f\"📋 Query: {result['query']}\")\n        print(f\"✅ Answer: {result['answer']}\")\n        print(\"-\" * 70)\n\n    print(\"\\n\" + \"=\" * 70)\n    print(\"All Queries Processed\")\n    print(\"=\" * 70 + \"\\n\")\n","metadata":{"id":"TVES3inSMlOn"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\n# Main Script\nfile_path = \"/content/document-one.txt\"\npersist_dir = \"/content/collections_one\"\n\n# Step 1: Initialize Embedding Model\nembeddings_model = LegalBERTEmbeddings()\n\n# Step 2: Load Files and Create Vector Store\ndoc_processor = DocumentProcessor(\n    collection_name=\"legal_docs_one\",\n    persist_directory=persist_dir,\n    embeddings_model=embeddings_model\n)\n\n# Run this only once to preprocess documents and store embeddings\ndoc_processor.load_files(file_path)\n\n# Step 3: Define Query System\nqa_chain = QAChain(document_processor=doc_processor)\n\n\nend_time = time.time()\n\n# Calculate the execution time\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"1KIxkYp4QK7T","outputId":"f2d24850-b5b7-45d0-dfd3-47f07bb2998f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"What is the location of the rental property?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"2b2Cw_T-c0VL","outputId":"8e5f3790-e924-4dab-a1ff-d46c3f450653"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"Who is the tenant?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"Mq5gDL8Jcz1Y","outputId":"f5a35ffc-afda-44c0-d6fd-09ae43de6480"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"Who is the landlord?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"kLaY1sKLcztK","outputId":"25261439-c4ed-4cac-9be6-7667021fe61a"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"What is the rental price?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"cB8DJt0Tczha","outputId":"17d0e588-57ba-405a-e833-8df1bddfe586"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"0MysrXLoIJJI"}},{"cell_type":"code","source":"start_time = time.time()\n\n# Main Script\nfile_path = \"/content/document-two.txt\"\npersist_dir = \"/content/collections_twoo\"\n\n# Step 1: Initialize Embedding Model\nembeddings_model = LegalBERTEmbeddings()\n\n# Step 2: Load Files and Create Vector Store\ndoc_processor = DocumentProcessor(\n    collection_name=\"legal_docs_two\",\n    persist_directory=persist_dir,\n    embeddings_model=embeddings_model\n)\n\n# Run this only once to preprocess documents and store embeddings\ndoc_processor.load_files(file_path)\n\n# Step 3: Define Query System\nqa_chain = QAChain(document_processor=doc_processor)\n\n\nend_time = time.time()\n\n# Calculate the execution time\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"Cxec_3xzRn-j","outputId":"8d0b242d-7565-43a9-e39b-4f53b24d80c9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"What is the location of the rental property?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"6AnVwhgNfEa6","outputId":"4a3f26b3-0ac1-476b-8cb7-9a46d6259737"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"Who is the tenant?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"vqBjDJcifELy","outputId":"620541bf-2c45-4790-aa33-3c39e950b0b9"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"Who is the landlord?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"yiA9LL_VfEI1","outputId":"c22b898c-a339-41b6-bfb9-a68d0e7593a3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"What is the rental price?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"QrAIAW2TfEFa","outputId":"3c349096-60ad-4f7b-ac6b-1603acd942f6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"What costs should the tenant carry?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"X65zGGk0fECS","outputId":"a486ee22-d598-4e71-a1ca-920d5b8e0f42"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\n\n# Main Script\nfile_path = \"/content/document-three.txt\"\npersist_dir = \"/content/collections_three\"\n\n# Step 1: Initialize Embedding Model\nembeddings_model = LegalBERTEmbeddings()\n\n# Step 2: Load Files and Create Vector Store\ndoc_processor = DocumentProcessor(\n    collection_name=\"legal_docs_three\",\n    persist_directory=persist_dir,\n    embeddings_model=embeddings_model\n)\n\n# Run this only once to preprocess documents and store embeddings\ndoc_processor.load_files(file_path)\n\n# Step 3: Define Query System\nqa_chain = QAChain(document_processor=doc_processor)\n\n\n\n\nend_time = time.time()\n\n# Calculate the execution time\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"pcnQXZf4Rn2E","outputId":"5497cd62-c9fb-46c5-dfa4-835a14bb5986"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"Which court should be used in case of disputes?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"lmXu0x2NSRM7","outputId":"7a30af55-5b4f-477a-fb17-e9d28c6a3a42"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"Which jurisdiction is relevant for this contract?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"fTiRUl7XSRJl","outputId":"5f31552a-4870-4be5-a9c6-0eb343005190"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"What are the IP transfer provisions?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"9RjyK9NXSRGN","outputId":"7712e5a5-7240-4e39-81c0-53e2293cde3f"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start_time = time.time()\nqueries = [\"Under what circumstances can the contract be terminated?\"]\n\nrun_independent_queries(queries)\n\nend_time = time.time()\n\n\nexecution_time = end_time - start_time\nprint(f\"Total Execution Time: {execution_time:.2f} seconds\")","metadata":{"id":"YpRc2V7CfdKX","outputId":"141dc8fc-11e5-41ee-91bf-dc79b9b92e4b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"TwhAXQpEuhGF"},"outputs":[],"execution_count":null}]}